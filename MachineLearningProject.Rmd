---
title: "Macnine Learning Project"
author: "DLKerski"
date: "April 13, 2019"
output: html_document
---
# Introduction

Data is from the Human Activity Recognition site, the Weight Lifting Exercise.  6 participants were asked to perform one set of 10 repetitions of the Unilateral Dumbell Biceps Curl in barbell lifts in five different fashions.  Exactly according to the lifting specification (Class A), and 4 others ways, where participants made common mistakes: throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E).  This exercise is to use the movement data to correctly identify the class for test members.  Data is the Weight Lifting Exercises Dataset (WLE), which measure arm movement, forearm movement, belt sensor orientation, and the dumbbell orientation sensors.  

# Data Preperation

Provided csv file are read into R
```{r eval=FALSE}
pmltest <-read.csv("pml-testing (1).csv")
pmltrain <-read.csv("pml-training (1).csv")
```

# Data set exploration, data reduction, and feature selection

Initial data set consist of 160 columns of data, with last column the classe of activity, A-D.  There are extended time periods defined, "data windows" of varying length.  The exact length of time perods is not clear from the data.  Data window "yes" was used to select data.  Using time windows decreased the number of observations from 19,623 to 407.  The time period involved is not clear, however standard deviations included showed similar numbers during time period for some activities, and wide variation for others.
```{r eval=FALSE}
#Select rows of data that were time period groupings.
pmltrainyes <-pmltrain[pmltrain$new_window %in% c("yes"),]
```
## Averages:
Investigation began with examination of averages available for movement data.

```{r eval=FALSE}
#Select columns that contained averages
pmltrainavg <-pmltrainyes[,grepl("avg",names(pmltrainyes))]
classe <- pmltrainyes$classe
username <-pmltrainyes$user_name
pmltrainavgclasse <- cbind(pmltrainavg,classe)
pmltrainavgclasseuser<- cbind(pmltrainavgclasse,username)
```

Using summary function for averages in each group there are 12 total, avg_roll_belt, avg_pitch_belt, avg_yaw_belt, avg_roll_arm, avg_pitch_arm, avg_yaw_arm, avg_roll_dumbbell, avg_pitch_dumbbell, avg_yaw_dumbbell, avg_roll_forearm, avg_pitch_forearm, and avg_yaw_forearm.  It could be assumed that performing a weight lifting activity incorrectly should generate different average values.

However using the featurePlot, while there were differences in the means for some activities, no clear pattern was evident.

```{r eval=FALSE}
featurePlot(x=pmltrainavgclasse[,c(1:12)],y=pmltrainavgclasse$classe,plot = "pairs", auto.key=list(columns=5))
```

## Activity and member were used as grouping looking for patterns in data

```{r eval=FALSE}
usernameavg <-pmltrainavgclasseuser %>% group_by(username,classe) %>% summarise_all("mean")
```

Examination was then attempted by user name, and classe of activity, averages of all groups did not show any clear patterns.  For example, it appeared for some activity's individuals had very similar values for many activities with no obvious pattern in data.


The entire data set was re-examined.  There were a large number of data columns with 1) little data, either zero's, blanks, or NA.  2) other fields such as skewness_yaw categories had most values are "DIV/0!". These were not examined.  Each data column was examined, and appropriate columns selected that contained data for all rows.

```{r eval=FALSE}
Pmltrainselect <-pmltrain[,c(8,9,10,11,37,38,39,40,41,42,43,44,45,46,47,48,49,60,61,62,63,64,65,66,67,68,84,85,86,102,113,114,115,116,117,118,119,120,121,122,123,124,140,151,152,153,154,155,156,157,158,159,160)]
```

# Decision Tree and Cross-validation
Given the large number of features available, individual variation, tree partitioning was attempted.  The training group was split into 70% for training with 30% for cross validation.

```{r eval=FALSE}
set.seed(1234)
train <-createDataPartition(y=pmltrainselect$classe, p=0.7, list = FALSE)
pmlselecttrain <-pmltrainselect[train,]
pmlselectval <- pmltrainselect[-train,]
pmlrpartfitcaret <- train(classe~.,data=pmlselecttrain, method="rpart")
pmlrpartfitcaret
```
The model accuracy was concerning

13737 samples
   52 predictor
    5 classes: 'A', 'B', 'C', 'D', 'E' 

No pre-processing
Resampling: Bootstrapped (25 reps) 
Summary of sample sizes: 13737, 13737, 13737, 13737, 13737, 13737, ... 
Resampling results across tuning parameters:

  cp          Accuracy   Kappa     
  0.03560167  0.5158190  0.37182338
  0.05953955  0.4210816  0.21870267
  0.11494253  0.3186809  0.05393258  
When used with the validation set, the confusion matrix showed even lower accuracy

```{r eval=FALSE}
pmlvalpredictcaret <-predict(pmlrpartfitcaret,pmlselectval)
confusionMatrix(pmlvalpredictcaret,pmlselectval$classe)

Confusion Matrix and Statistics

          Reference
Prediction    A    B    C    D    E
         A 3574 1117 1124 1021  370
         B   58  891   82  376  339
         C  267  650 1190  855  679
         D    0    0    0    0    0
         E    7    0    0    0 1137

Overall Statistics
                                         
               Accuracy : 0.4944         
                 95% CI : (0.486, 0.5028)
    No Information Rate : 0.2843         
    P-Value [Acc > NIR] : < 2.2e-16      
                                         
                  Kappa : 0.3389         
 Mcnemar's Test P-Value : NA             

Statistics by Class:

                     Class: A Class: B Class: C Class: D Class: E
Sensitivity            0.9150  0.33521  0.49666   0.0000  0.45030
Specificity            0.6306  0.92283  0.78388   1.0000  0.99938
Pos Pred Value         0.4960  0.51031  0.32683      NaN  0.99388
Neg Pred Value         0.9492  0.85264  0.88055   0.8361  0.88978
Prevalence             0.2843  0.19349  0.17442   0.1639  0.18381
Detection Rate         0.2602  0.06486  0.08663   0.0000  0.08277
Detection Prevalence   0.5246  0.12710  0.26505   0.0000  0.08328
Balanced Accuracy      0.7728  0.62902  0.64027   0.5000  0.72484

For the validation group the overall model accuracy was close to .5, with classe B and E poorly identified, and there were groups such as classe D with zero Sensitivity as none were correctly identified.

# Random Forest Model
Because of this a random forest model was generated using all the training data.  No preprocessing or specific cross validation step is used, as random forest is an ensemble method, where intrinsically the training algorithm applies bootstrap aggregating, or bagging, as part of this process to trees generated.  The model generated showed improved accuracy.

```{r eval=FALSE}
pmlrandomfmodFit <-train(classe ~., data= pmltrainselect, method = "rf")
pmlrandomfmodFit
```


mtry  Accuracy   Kappa    
   2    0.9928453  0.9909472
  27    0.9925515  0.9905751
  52    0.9846474  0.9805724

The first model was chosen by the algorithm, and prediction from this random forest model,

```{r eval=FALSE}
pmlrftest <-predict(pmlrandomfmodFit,pmltest)
pmlrftest
```
B A B A A E D B A A B C B A E E A B B B